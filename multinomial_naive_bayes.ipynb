{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes on PyTorch\n",
    "\n",
    "Below is a very simple demonstration of a multinomial Naive Bayes using PyTorch. It's very simple and doesn't include any input checking or advanced features. When the algorithm is moved over the cuML, it will also require that we have a `LabelBinarizer` on GPU. The Naive Bayes MNMG version will require that we have an MNMG version of `LabelBinarizer`. \n",
    "\n",
    "\n",
    "### A note on MNMG design\n",
    "Since we only need the marginals of the classes for the prior and\n",
    "the marginals of each term / feature, this algorithm is extremely\n",
    "simple to parallelize over a cluster of workers. \n",
    "\n",
    "Assuming the set of terms is small enough to fit on each GPU:\n",
    "\n",
    "1. The labels will first need to be one-hot encoded, which would require\n",
    "taking the global union of the set of labels across the cluster, \n",
    "then the workers can binarize individually. \n",
    "\n",
    "2. After one-hot encoding the labels, the workers can compute the \n",
    "frequencies for the classes and the features. \n",
    "\n",
    "3. The frequencies are reduced on a single worker, who follows the \n",
    "remaining steps from the single GPU `train()`\n",
    "\n",
    "4. For inference, the model is propagated out to all the workers with\n",
    "inference partitions for embarrassingly parallel inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2257x35788 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 365886 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "\n",
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def scipy_to_torch(sp):\n",
    "    coo = sp.tocoo()\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "    \n",
    "    i = torch.cuda.LongTensor(indices)\n",
    "    v = torch.cuda.FloatTensor(values)\n",
    "\n",
    "    return torch.cuda.sparse.FloatTensor(i, v, torch.Size(coo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scipy_to_torch(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultinomialNB(object):\n",
    "    \n",
    "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "        \n",
    "        self.n_features_ = None\n",
    "        \n",
    "    def fit(self, X, y, _partial=False, _classes=None):\n",
    "\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self._init_counters(y.shape[1], X.shape[1])\n",
    "\n",
    "        self.classes_ = l.classes_\n",
    "        self.n_classes_ = len(l.classes_)\n",
    "\n",
    "        self._count(X, y)\n",
    "        self._update_feature_log_prob(self.alpha)\n",
    "        self._update_class_log_prior(class_prior=self.class_prior)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        \n",
    "        _, indices = torch.max(jll, 1)\n",
    "        return indices\n",
    "        \n",
    "    def _init_counters(self, n_effective_classes, n_features):\n",
    "        self.class_count_ = torch.zeros(n_effective_classes).cuda()\n",
    "        self.feature_count_ = torch.zeros(n_effective_classes, n_features).cuda()\n",
    "        \n",
    "\n",
    "    def _count(self, X, Y):\n",
    "        self.feature_count_ += torch.sparse.mm(X.t(), Y).t()\n",
    "        self.class_count_ += Y.sum(axis=0)\n",
    "        \n",
    "    def _update_class_log_prior(self, class_prior=None):\n",
    "\n",
    "        if class_prior is not None:\n",
    "            self.class_log_prior_ = torch.log(class_prior)\n",
    "            \n",
    "        elif self.fit_prior:\n",
    "            log_class_count = torch.log(self.class_count_)\n",
    "\n",
    "        self.class_log_prior_ = torch.full((self.n_classes_,1), \n",
    "                                           -math.log(self.n_classes_)).cuda()\n",
    "        \n",
    "    def _update_feature_log_prob(self, alpha):\n",
    "        \"\"\" apply add-lambda smoothing to raw counts and recompute log probabilities\"\"\"\n",
    "        smoothed_fc = self.feature_count_ + alpha\n",
    "        smoothed_cc = smoothed_fc.sum(axis=1).reshape(-1, 1)\n",
    "        self.feature_log_prob_ = (torch.log(smoothed_fc) - torch.log(smoothed_cc))\n",
    "        \n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\" Calculate the posterior log probability of the samples X \"\"\"\n",
    "        ret = torch.sparse.mm(X, self.feature_log_prob_.T)\n",
    "        ret += self.class_log_prior_.T\n",
    "        return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LabelBinarizer()\n",
    "Y = torch.cuda.FloatTensor(l.fit_transform(twenty_train.target)).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 ms, sys: 86 µs, total: 26.3 ms\n",
      "Wall time: 10.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MultinomialNB at 0x7feb28b16a58>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "m = MultinomialNB()\n",
    "m.fit(a, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.13 ms, sys: 4.55 ms, total: 6.67 ms\n",
      "Wall time: 5.89 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_hat_gpu = m.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9964554718653079"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_hat_gpu.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 ms, sys: 632 µs, total: 11.5 ms\n",
      "Wall time: 10.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sklearn_model = MNB()\n",
    "sklearn_model.fit(X_train_counts, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.09 ms, sys: 0 ns, total: 5.09 ms\n",
      "Wall time: 4.29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_hat = sklearn_model.predict(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9964554718653079"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = y - y_hat_gpu.cpu().numpy()\n",
    "len(results[results!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuml)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
